\section{Euclidean Spaces}\label{sec:euclidean-spaces}

So far, all we've talked about is single dimensional spaces. Now technically we
haven't defined what dimensions are, and surprisingly, it's actually \emph{very}
difficult to understand dimension at an abstract level, but we should have
somewhat of an idea as we live in 3 spatial dimensions and 1 time dimension.

In this section we'll lay some groundwork for starting to do some work in more
than one dimension. Mathematics shouldn't usually be done for the sake of
applications, but if you're that kind of person, you'll love this section.

\begin{definition}\label{def:Rn}
    For any positive integer $n$ we will define $\R^n$ to be the set of all
    ordered $n$-tuples
    \begin{equation*}
        \mathbf{x} = \left(x_1, x_2, \ldots, x_n\right).
    \end{equation*}
    These objects are often called \emph{vectors}, or points in $\R^n$, and we
    use boldface to distinguish them from one dimensional objects. We call
    $x_1, x_2, \ldots, x_n$ the \emph{coordinates} of $\mathbf{x}$.
\end{definition}

We can then define addition, and scalar multiplication on these objects as
\begin{align}
    \mathbf{x} + \mathbf{y} & \coloneqq \left(x_1 + y_1, x_2 + y_2, \ldots, + x_n + y_n\right)  \\
    \alpha\mathbf{x}        & \coloneqq \left(\alpha x_1, \alpha x_2, \ldots, \alpha x_n\right)
\end{align}
where $\alpha\in\R$. These two properties satisfy all the nice properties that
we would expect and want from an object we plan to use often; you know things
like commutativity ($a + b = b + a$), associativity ($(a + b) + c = a + (b + c)$),
and distributivity ($a(b + c) = ab + ac$). These properties we so often take for
granted in $\R^1$ are hence transferred into $\R^n$ which makes it into a
\emph{vector space of the real field}\footnote{This is technical jargon, and if
    you don't understand it now, don't worry, we won't assume you do. That said if
    you do you what it means, it might help you bring along some knowledge to your
    understanding.}. A vector space must always have a ``zero'' element which does
nothing if added to something else\footnote{($a + 0 = a$)}. In $\R^n$ we call this
element $\mathbf{0}$ and define it as the point with all coordinates equal to 0.

Rudin here defines this thing called the ``inner product'' and it kind of just
comes out of nowhere, and if you're unfamiliar with it, it could be quite
bizarre.\todo{motivate inner products}

\begin{equation}\label{eq:innerproduct}
    \mathbf{x}\cdot\mathbf{y}\coloneqq\sum_{i=1}^k x_i y_i
\end{equation}

\begin{equation}\label{eq:norm}
    \|\mathbf{x}\|\coloneqq\sqrt{\mathbf{x}\cdot\mathbf{x}} = \left(\sum_{i=1}^k x_i^2\right)^{1/2}
\end{equation}

Let's not prove some very basic theorems using~\ref{eq:innerproduct} and~\ref{eq:norm}.

\begin{theorem}
    If $\mathbf{x}, \mathbf{y}, \mathbf{z}\in\R^n$, and $\alpha\in\R$ then
    \begin{itemize}
        \item $\|\mathbf{x}\| \geq 0$;
        \item $\|\mathbf{x}\| = 0$ if and only if $\mathbf{x} = \mathbf{0}$;
        \item $\|\alpha\mathbf{x}\| = |\alpha|\|\mathbf{x}\|$;
        \item $\|\mathbf{x}\cdot\mathbf{y}\| \leq \|\mathbf{x}\|\|\mathbf{y}\|$;
        \item $\|\mathbf{x} + \mathbf{y}\| \leq \|\mathbf{x}\| + \|\mathbf{y}\|$;
        \item $\|\mathbf{x} - \mathbf{z}\| \leq \|\mathbf{x} - \mathbf{y}\| + \|\mathbf{y} - \mathbf{z}\|$.
    \end{itemize}
\end{theorem}
\begin{proof}
    Rudin thinks the first four are obvious, but we will show them here since we
    are still just getting warmed up.
    \begin{itemize}
        \item[(a)] If we sum the squares of a bunch of numbers\footnote{\ref{eq:norm}},
            we get something positive, and taking the square root of that will also
            always be postives.
        \item[(b)] Since each $x_i^2 \geq 0$, if we want to sum them to 0, they
            must all be 0. This is exactly the definition of $\mathbf{0}$ we saw
            earlier.
        \item[(c)]
            \begin{align}
                \|\alpha\mathbf{x}\| & = \left(\sum_{i=1}^k (\alpha x_i)^2\right)^{1/2} \\
                                     & = \left(\sum_{i=1}^k \alpha^2 x_i^2\right)^{1/2} \\
                                     & = \left(\alpha^2\sum_{i=1}^k x_i^2\right)^{1/2}  \\
                                     & = \alpha\left(\sum_{i=1}^k x_i^2\right)^{1/2}    \\
                                     & = \alpha\|\mathbf{x}\|
            \end{align}
        \item[(d)] As we saw in the previous section\todo{hasn't been written yet}
        \item[(e)] For this proof lets start off by squaring the left hand side
            and doing some manipulations, and then also use a fair amount of (d).
            \begin{align}
                \|\mathbf{x} + \mathbf{y}\|^2 & = (\mathbf{x} + \mathbf{y})\cdot(\mathbf{x} + \mathbf{y})                            \\
                                              & = \mathbf{x}\cdot\mathbf{x} + 2\mathbf{x}\cdot\mathbf{y} + \mathbf{y}\cdot\mathbf{y} \\
                                              & \leq \|\mathbf{x}\|^2 + 2\|\mathbf{x}\|\|\mathbf{y}\| + \|\mathbf{y}\|^2             \\
                                              & = \left(\|\mathbf{x}\| + \|\mathbf{y}\|\right)^2
            \end{align}
            We can now take the square root of both sides (which we can do since we
            know both sides will be positive) to obtain the desired result.
        \item[(f)] Since we've proved (e) we can send $\mathbf{x}\to\mathbf{x} - \mathbf{y}$
            and $\mathbf{y}\to\mathbf{y} - \mathbf{z}$. Under this substitution we
            get the formula we were trying to prove. But this result is much more
            than just a formula because it's really about the geometry.
            \begin{center}
                \begin{tikzpicture}
                    \draw (0,0) -- (4,1) node[right] {$\mathbf{y}$}
                    -- (3,4) node[above] {$\mathbf{x}$}
                    -- (0,0) node[left] {$\mathbf{z}$};
                \end{tikzpicture}
            \end{center}
            As we can see from the picture the distance between $\mathbf{x}$ and
            $\mathbf{x}$ (which is what $\|\mathbf{x} - \mathbf{z}\|$ means) will
            always be shorter than the distance from $\mathbf{x}$ to $\mathbf{y}$
            \emph{plus} the distance from $\mathbf{y}$ to $\mathbf{z}$.
    \end{itemize}
\end{proof}

As we probably colloqiually know, $\R^1$ is known as the real line, and $\R^2$
the plane. But here Rudin also suggests we call $\R^2$ the complex plane which
is kind of odd. We defined $\R^2$ to be the two-tuples of \emph{real} numbers!
Where are the complex ones coming from? Well as we saw in\todo{section not written yet}
$\C$ is really just $\R^2$ with a different vector multiplication.

